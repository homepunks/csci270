<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-c51093a4.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-9bddd779.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="lecture-1-introduction--analysis-of-algorithms"><a class="header" href="#lecture-1-introduction--analysis-of-algorithms">Lecture 1: Introduction &amp; Analysis of Algorithms</a></h1>
<h2 id="what-is-an-algorithm"><a class="header" href="#what-is-an-algorithm">What is an Algorithm?</a></h2>
<p>An <strong>algorithm</strong> is a set of precise, step-by-step instructions designed to solve a specific problem or perform a defined task. It must be unambiguous and detailed enough to be executed by a computer.</p>
<h3 id="characteristics-of-an-algorithm"><a class="header" href="#characteristics-of-an-algorithm">Characteristics of an Algorithm</a></h3>
<ol>
<li><strong>Input</strong>: Must take zero or more inputs.</li>
<li><strong>Output</strong>: Must produce at least one output (e.g., a value, a decision).</li>
<li><strong>Definiteness</strong>: Each instruction must be clear and unambiguous.</li>
<li><strong>Finiteness</strong>: The algorithm must terminate after a finite number of steps.</li>
<li><strong>Effectiveness</strong>: Every operation must be basic enough to be performed exactly and in a finite amount of time.</li>
</ol>
<h2 id="analysis-of-algorithms"><a class="header" href="#analysis-of-algorithms">Analysis of Algorithms</a></h2>
<p><strong>Analysis of algorithms</strong> is the theoretical study of the performance and resource usage (time, memory) of computer programs. While performance is important, other factors are equally or more critical in software engineering:</p>
<ul>
<li><strong>Correctness</strong></li>
<li><strong>Modularity</strong></li>
<li><strong>Maintainability</strong></li>
<li><strong>Functionality</strong></li>
<li><strong>Robustness</strong></li>
<li><strong>User-friendliness</strong></li>
<li><strong>Programmer time</strong></li>
<li><strong>Simplicity</strong></li>
<li><strong>Extensibility</strong></li>
<li><strong>Reliability</strong></li>
</ul>
<h3 id="why-study-algorithms-and-performance"><a class="header" href="#why-study-algorithms-and-performance">Why Study Algorithms and Performance?</a></h3>
<ul>
<li>Algorithms help us understand <strong>scalability</strong>.</li>
<li>Performance often determines the <strong>feasibility</strong> of solving a problem.</li>
<li>Algorithmic mathematics provides a language for describing <strong>program behavior</strong>.</li>
<li>Performance is the <strong>currency of computing</strong>—lessons about performance generalize to other computing resources.</li>
</ul>
<h2 id="the-sorting-problem"><a class="header" href="#the-sorting-problem">The Sorting Problem</a></h2>
<p><strong>Input</strong>: A sequence of numbers:<br><code>&lt;a₁, a₂, ..., aₙ&gt;</code></p>
<p><strong>Output</strong>: A permutation (reordering) of the input sequence:<br><code>&lt;a′₁, a′₂, ..., a′ₙ&gt;</code><br>such that:<br><code>a′₁ ≤ a′₂ ≤ ... ≤ a′ₙ</code></p>
<p><strong>Example</strong>:<br>Input: <code>8 2 4 9 3 6</code><br>Output: <code>2 3 4 6 8 9</code></p>
<h2 id="insertion-sort"><a class="header" href="#insertion-sort">Insertion Sort</a></h2>
<p>Insertion Sort is a simple, iterative sorting algorithm that builds the final sorted array one element at a time.</p>
<h3 id="pseudocode"><a class="header" href="#pseudocode">Pseudocode</a></h3>
<pre><code>INSERTION-SORT(A, n)  // A[1...n]
for j = 2 to n
    key = A[j]
    i = j - 1
    while i &gt; 0 and A[i] &gt; key
        A[i+1] = A[i]
        i = i - 1
    A[i+1] = key
</code></pre>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<ol>
<li>Start with the second element (<code>j = 2</code>).</li>
<li>Compare it with the elements before it (left side), which are already sorted.</li>
<li>Shift all larger elements one position to the right.</li>
<li>Insert the current element into its correct position.</li>
</ol>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p>Input: <code>8 2 4 9 3 6</code><br>Step-by-step:</p>
<ol>
<li><code>2 8 4 9 3 6</code></li>
<li><code>2 4 8 9 3 6</code></li>
<li><code>2 4 8 9 3 6</code></li>
<li><code>2 3 4 8 9 6</code></li>
<li><code>2 3 4 6 8 9</code></li>
</ol>
<h2 id="running-time-analysis"><a class="header" href="#running-time-analysis">Running Time Analysis</a></h2>
<p>The <strong>running time</strong> of an algorithm depends on:</p>
<ol>
<li><strong>Input size</strong>: Longer sequences take more time to sort.</li>
<li><strong>Input order</strong>: Already sorted sequences are easier to sort.</li>
</ol>
<p>We typically analyze the <strong>worst-case</strong> running time to guarantee performance.</p>
<h3 id="types-of-analysis"><a class="header" href="#types-of-analysis">Types of Analysis</a></h3>
<ul>
<li><strong>Worst-case</strong>: <code>T(n) = maximum time on any input of size n</code></li>
<li><strong>Average-case</strong>: <code>T(n) = expected time over all inputs of size n</code> (requires a statistical distribution assumption)</li>
<li><strong>Best-case</strong>: Often misleading—a slow algorithm may perform well on specific inputs.</li>
</ul>
<h3 id="asymptotic-analysis"><a class="header" href="#asymptotic-analysis">Asymptotic Analysis</a></h3>
<p>We focus on <strong>growth rates</strong> as <code>n → ∞</code>, ignoring machine-dependent constants. This is called <strong>asymptotic analysis</strong>.</p>
<h2 id="asymptotic-notation-θ-theta"><a class="header" href="#asymptotic-notation-θ-theta">Asymptotic Notation: Θ (Theta)</a></h2>
<p>The <strong>Theta notation</strong> provides a tight bound on a function’s growth rate.</p>
<p><strong>Mathematical Definition</strong>:<br><code>Θ(g(n)) = { f(n): there exist positive constants c₁, c₂, and n₀ such that 0 ≤ c₁·g(n) ≤ f(n) ≤ c₂·g(n) for all n ≥ n₀ }</code></p>
<p><strong>Engineering Interpretation</strong>:<br>Drop lower-order terms and leading constants.</p>
<p><strong>Example</strong>:<br><code>3n³ + 90n² − 5n + 6046 = Θ(n³)</code></p>
<h2 id="asymptotic-performance"><a class="header" href="#asymptotic-performance">Asymptotic Performance</a></h2>
<ul>
<li>When <code>n</code> is large, a <code>Θ(n²)</code> algorithm outperforms a <code>Θ(n³)</code> algorithm.</li>
<li>However, asymptotically slower algorithms may still be useful in practice where constants or lower-order terms matter.</li>
<li>Asymptotic analysis helps structure our thinking about algorithm efficiency.</li>
</ul>
<h2 id="insertion-sort-analysis"><a class="header" href="#insertion-sort-analysis">Insertion Sort Analysis</a></h2>
<p><strong>Worst-case</strong>: Input is reverse sorted.<br>The running time is:<br><code>T(n) = Σ(from j=2 to n) Θ(j) = Θ(n²)</code> (arithmetic series)</p>
<h3 id="is-insertion-sort-fast"><a class="header" href="#is-insertion-sort-fast">Is Insertion Sort Fast?</a></h3>
<ul>
<li><strong>Moderately fast</strong> for small <code>n</code>.</li>
<li><strong>Inefficient</strong> for large <code>n</code>.</li>
</ul>
<hr>
<h2 id="practice-tasks"><a class="header" href="#practice-tasks">Practice Tasks</a></h2>
<ol>
<li>
<p><strong>Algorithm Characteristics</strong><br>Explain why each characteristic of an algorithm (input, output, definiteness, finiteness, effectiveness) is necessary.</p>
</li>
<li>
<p><strong>Insertion Sort Execution</strong><br>Trace Insertion Sort step-by-step on the input sequence: <code>5 1 4 2 8</code>. Show the array after each iteration of the outer loop.</p>
</li>
<li>
<p><strong>Running Time Scenarios</strong><br>For Insertion Sort:</p>
<ul>
<li>What is the best-case input? What is its running time?</li>
<li>What is the worst-case input? What is its running time?</li>
</ul>
</li>
<li>
<p><strong>Asymptotic Notation</strong><br>Prove or disprove:<br>a) <code>7n³ + 2n² + 100 = Θ(n³)</code><br>b) <code>n² + 50n log n = Θ(n²)</code></p>
</li>
<li>
<p><strong>Comparative Analysis</strong><br>Suppose Algorithm A runs in <code>Θ(n log n)</code> and Algorithm B runs in <code>Θ(n²)</code>. For what values of <code>n</code> would you prefer Algorithm B if it has much smaller constant factors? Explain.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lecture-2-asymptotic-notation--algorithm-analysis"><a class="header" href="#lecture-2-asymptotic-notation--algorithm-analysis">Lecture 2: Asymptotic Notation &amp; Algorithm Analysis</a></h1>
<h2 id="recap-analysis-of-algorithms"><a class="header" href="#recap-analysis-of-algorithms">Recap: Analysis of Algorithms</a></h2>
<p><strong>Analysis of algorithms</strong> is the theoretical study of computer program performance and resource usage. While performance is important, other software engineering qualities are equally or more critical:</p>
<ul>
<li><strong>Correctness</strong>: Does the algorithm produce the right output?</li>
<li><strong>Modularity</strong>: Is the design composed of independent, interchangeable components?</li>
<li><strong>Maintainability</strong>: How easy is it to modify and update the algorithm?</li>
<li><strong>Functionality</strong>: Does it meet the specified requirements?</li>
<li><strong>Robustness</strong>: How well does it handle invalid inputs or unexpected conditions?</li>
<li><strong>User-friendliness</strong>: Is the interface intuitive?</li>
<li><strong>Programmer time</strong>: How long does it take to implement?</li>
<li><strong>Simplicity</strong>: Is the design straightforward?</li>
<li><strong>Extensibility</strong>: Can it be easily extended with new features?</li>
<li><strong>Reliability</strong>: Does it perform consistently under various conditions?</li>
</ul>
<h2 id="recap-insertion-sort"><a class="header" href="#recap-insertion-sort">Recap: Insertion Sort</a></h2>
<p>Insertion Sort is a simple comparison-based sorting algorithm that builds the final sorted array one element at a time.</p>
<h3 id="pseudocode-1"><a class="header" href="#pseudocode-1">Pseudocode</a></h3>
<pre><code>INSERTION-SORT(A, n)  // A[1...n]
for j ← 2 to n
    key ← A[j]
    i ← j - 1
    while i &gt; 0 and A[i] &gt; key
        A[i+1] ← A[i]
        i ← i - 1
    A[i+1] ← key
</code></pre>
<h3 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h3>
<ol>
<li>Start from the second element (index 2).</li>
<li>Compare it with elements to its left.</li>
<li>Shift larger elements right to make space.</li>
<li>Insert the current element into its correct position.</li>
</ol>
<h2 id="recap-types-of-analysis"><a class="header" href="#recap-types-of-analysis">Recap: Types of Analysis</a></h2>
<h3 id="worst-case-analysis"><a class="header" href="#worst-case-analysis">Worst-case Analysis</a></h3>
<p><code>T(n) = maximum time of algorithm on any input of size n</code></p>
<ul>
<li>Most common analysis method</li>
<li>Provides a performance guarantee</li>
</ul>
<h3 id="average-case-analysis"><a class="header" href="#average-case-analysis">Average-case Analysis</a></h3>
<p><code>T(n) = expected time over all inputs of size n</code></p>
<ul>
<li>Requires assumption about statistical distribution of inputs</li>
<li>More realistic for many applications</li>
</ul>
<h3 id="best-case-analysis"><a class="header" href="#best-case-analysis">Best-case Analysis</a></h3>
<ul>
<li>Often misleading</li>
<li>Can “cheat” by designing algorithms that work well only on specific inputs</li>
</ul>
<h2 id="asymptotic-notations"><a class="header" href="#asymptotic-notations">Asymptotic Notations</a></h2>
<p>Asymptotic notations describe the growth rate of functions, particularly useful for comparing algorithm running times.</p>
<h3 id="o-notation-big-o---upper-bound"><a class="header" href="#o-notation-big-o---upper-bound">O-notation (Big-O) - Upper Bound</a></h3>
<p><code>O(g(n))</code> represents the set of functions that grow <strong>no faster than</strong> <code>g(n)</code>.</p>
<p><strong>Formal Definition</strong>:<br><code>O(g(n)) = { f(n): ∃ constants c &gt; 0, n₀ &gt; 0 such that 0 ≤ f(n) ≤ c·g(n) ∀ n ≥ n₀ }</code></p>
<p><strong>Example</strong>: <code>2n² ∈ O(n³)</code><br>We can choose <code>c = 1</code> and <code>n₀ = 2</code>:</p>
<ul>
<li><code>0 ≤ 2n² ≤ n³</code> for all <code>n ≥ 2</code></li>
</ul>
<h3 id="ω-notation-omega---lower-bound"><a class="header" href="#ω-notation-omega---lower-bound">Ω-notation (Omega) - Lower Bound</a></h3>
<p><code>Ω(g(n))</code> represents the set of functions that grow <strong>at least as fast as</strong> <code>g(n)</code>.</p>
<p><strong>Formal Definition</strong>:<br><code>Ω(g(n)) = { f(n): ∃ constants c &gt; 0, n₀ &gt; 0 such that 0 ≤ c·g(n) ≤ f(n) ∀ n ≥ n₀ }</code></p>
<p><strong>Example</strong>: <code>√n = Ω(log n)</code><br>We can choose <code>c = 1</code> and <code>n₀ = 16</code>:</p>
<ul>
<li><code>0 ≤ 1·log n ≤ √n</code> for all <code>n ≥ 16</code></li>
</ul>
<h3 id="θ-notation-theta---tight-bound"><a class="header" href="#θ-notation-theta---tight-bound">Θ-notation (Theta) - Tight Bound</a></h3>
<p><code>Θ(g(n))</code> represents the set of functions that grow <strong>at the same rate as</strong> <code>g(n)</code>.</p>
<p><strong>Formal Definition</strong>:<br><code>Θ(g(n)) = { f(n): ∃ positive constants c₁, c₂, n₀ such that 0 ≤ c₁·g(n) ≤ f(n) ≤ c₂·g(n) ∀ n ≥ n₀ }</code></p>
<p><strong>Equivalence</strong>: <code>Θ(g(n)) = O(g(n)) ∩ Ω(g(n))</code></p>
<p><strong>Example</strong>: <code>½n² - 2n = Θ(n²)</code></p>
<h2 id="examples-of-asymptotic-relationships"><a class="header" href="#examples-of-asymptotic-relationships">Examples of Asymptotic Relationships</a></h2>
<h3 id="proving-bounds"><a class="header" href="#proving-bounds">Proving Bounds</a></h3>
<ol>
<li>
<p><strong>n³ + 20n + 1 is O(n³)</strong><br>For large n, n³ dominates.</p>
</li>
<li>
<p><strong>n³ + 20n + 1 is not O(n²)</strong><br>The cubic term grows faster than any constant multiple of n².</p>
</li>
<li>
<p><strong>n³ + 20n + 1 is O(n⁴)</strong><br>True, but not a tight bound.</p>
</li>
<li>
<p><strong>2n³ - 7n + 1 = Ω(n³)</strong><br>The leading term ensures growth at least as fast as n³.</p>
</li>
<li>
<p><strong>n³ + 20n = Ω(n²)</strong><br>Cubic growth exceeds quadratic.</p>
</li>
</ol>
<h3 id="important-notes"><a class="header" href="#important-notes">Important Notes</a></h3>
<ul>
<li>The choice of <code>n₀</code> and <code>c</code> is <strong>not unique</strong></li>
<li>Multiple valid combinations can prove the same relationship</li>
<li>The proof depends on the inequalities used for bounding</li>
</ul>
<h2 id="common-function-classes-in-asymptotic-notation"><a class="header" href="#common-function-classes-in-asymptotic-notation">Common Function Classes in Asymptotic Notation</a></h2>
<h3 id="functions-in-on²"><a class="header" href="#functions-in-on²">Functions in O(n²)</a></h3>
<pre><code>n²
n² + n
n² + 1000n
1000n² + 1000n
n
n/1000
n¹·⁹⁹⁹⁹⁹
n²/(log log log n)
</code></pre>
<h3 id="functions-in-ωn²"><a class="header" href="#functions-in-ωn²">Functions in Ω(n²)</a></h3>
<pre><code>n²
n² + n
n² - n
1000n² + 1000n
1000n² - 1000n
n³
n²·⁰⁰⁰⁰¹
n² log log log n
2^(2^n)
</code></pre>
<h2 id="analyzing-algorithm-running-time"><a class="header" href="#analyzing-algorithm-running-time">Analyzing Algorithm Running Time</a></h2>
<h3 id="example-nested-loops"><a class="header" href="#example-nested-loops">Example: Nested Loops</a></h3>
<pre><code class="language-python">def foo(lst):
    result1 = 1
    result2 = 6
    for i in range(len(lst)):
        for j in range(len(lst)):
            result1 += i * j
            result2 = lst[j] + i
    return
</code></pre>
<p><strong>Analysis</strong>:</p>
<ul>
<li>Let <code>n = len(lst)</code></li>
<li>Outer loop: <code>n</code> iterations</li>
<li>Inner loop: <code>n</code> iterations per outer iteration</li>
<li>Each inner iteration: constant time (2 operations)</li>
<li>Lines 1-2: constant time (2 operations)</li>
<li>Total: <code>2 + n × n × 2 = 2n² + 2 = Θ(n²)</code></li>
</ul>
<h2 id="growth-rate-ranking-of-functions"><a class="header" href="#growth-rate-ranking-of-functions">Growth Rate Ranking of Functions</a></h2>
<p>From fastest to slowest growth:</p>
<pre><code>nⁿ           (fastest)
2ⁿ
n³
n²
n log n
n
√n
log n
1           (slowest)
</code></pre>
<p><strong>Key Insight</strong>:<br><code>n log n</code> grows faster than <code>n</code> but slower than <code>n²</code>. This explains why <code>Θ(n log n)</code> sorting algorithms (like Merge Sort) are more efficient than <code>Θ(n²)</code> algorithms (like Insertion Sort) for large inputs.</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<ol>
<li>
<p><strong>Function Description</strong>: We use functions to describe algorithm runtime as a function of input size <code>n</code>.</p>
</li>
<li>
<p><strong>Asymptotic Notations</strong>:</p>
<ul>
<li><code>O</code> for upper bounds (≤)</li>
<li><code>Ω</code> for lower bounds (≥)</li>
<li><code>Θ</code> for tight bounds (=)</li>
</ul>
</li>
<li>
<p><strong>Proof Technique</strong>: Proving asymptotic relationships involves finding constants <code>n₀</code> and <code>c</code> that satisfy the inequality chain.</p>
</li>
<li>
<p><strong>Practical Application</strong>: Asymptotic analysis helps compare algorithms independent of hardware and implementation details.</p>
</li>
</ol>
<hr>
<h2 id="practice-tasks-1"><a class="header" href="#practice-tasks-1">Practice Tasks</a></h2>
<h3 id="task-1-formal-proofs"><a class="header" href="#task-1-formal-proofs">Task 1: Formal Proofs</a></h3>
<p>Prove the following using formal definitions:</p>
<ol>
<li><code>3n³ + 2n² + 5 = O(n³)</code></li>
<li><code>n² + 100n = Ω(n²)</code></li>
<li><code>2n² - n = Θ(n²)</code></li>
</ol>
<h3 id="task-2-true-or-false"><a class="header" href="#task-2-true-or-false">Task 2: True or False</a></h3>
<p>Determine if each statement is true or false. Justify your answer.</p>
<ol>
<li><code>n³ = O(n²)</code></li>
<li><code>2ⁿ = Ω(n¹⁰⁰)</code></li>
<li><code>log n = O(√n)</code></li>
<li><code>n! = O(nⁿ)</code></li>
<li><code>n² log n = Ω(n²)</code></li>
</ol>
<h3 id="task-3-algorithm-analysis"><a class="header" href="#task-3-algorithm-analysis">Task 3: Algorithm Analysis</a></h3>
<p>Analyze the time complexity of the following pseudocode:</p>
<pre><code>function mystery(A, n):
    sum = 0
    for i = 1 to n:
        for j = 1 to i:
            sum = sum + A[i] * A[j]
    return sum
</code></pre>
<h3 id="task-4-function-classification"><a class="header" href="#task-4-function-classification">Task 4: Function Classification</a></h3>
<p>Classify each function into the most specific asymptotic class:</p>
<ol>
<li><code>f(n) = 5n³ + 2n log n + 7</code></li>
<li><code>f(n) = 100n + √n + log n</code></li>
<li><code>f(n) = n² log n + n(log n)²</code></li>
<li><code>f(n) = 2ⁿ + n³⁰</code></li>
<li><code>f(n) = n! + 2ⁿ</code></li>
</ol>
<h3 id="task-5-practical-implications"><a class="header" href="#task-5-practical-implications">Task 5: Practical Implications</a></h3>
<p>Suppose you have two algorithms:</p>
<ul>
<li>Algorithm A: <code>T(n) = 100n²</code></li>
<li>Algorithm B: <code>T(n) = 2n³</code></li>
</ul>
<ol>
<li>For what values of <code>n</code> is Algorithm A faster than Algorithm B?</li>
<li>If you expect <code>n ≤ 50</code>, which algorithm would you choose?</li>
<li>How would your choice change if <code>n</code> could be up to 500?</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lecture-3-asymptotic-notation-divide--conquer-recurrence-relations"><a class="header" href="#lecture-3-asymptotic-notation-divide--conquer-recurrence-relations">Lecture 3: Asymptotic Notation, Divide &amp; Conquer, Recurrence Relations</a></h1>
<h2 id="asymptotic-notation-review"><a class="header" href="#asymptotic-notation-review">Asymptotic Notation Review</a></h2>
<p>Asymptotic notation provides a way to compare the <strong>growth rates</strong> of functions, which is essential for analyzing algorithm running times. Just as we compare numbers using relational operators (≤, ≥, =, &lt;, &gt;), we compare function growth rates using asymptotic notations.</p>
<h3 id="comparing-functions-vs-numbers"><a class="header" href="#comparing-functions-vs-numbers">Comparing Functions vs. Numbers</a></h3>
<p><strong>For numbers:</strong></p>
<ul>
<li><code>x ≤ 10</code>: x is at most 10</li>
<li><code>x ≥ 10</code>: x is at least 10</li>
<li><code>x ≤ 6</code> and <code>x ≥ 6</code> implies <code>x = 6</code></li>
<li><code>x &lt; 5</code>: x is strictly less than 5</li>
</ul>
<p><strong>For functions (growth rates):</strong></p>
<ul>
<li><code>f(n) = O(g(n))</code>: f grows at most as fast as g (similar to ≤)</li>
<li><code>f(n) = Ω(g(n))</code>: f grows at least as fast as g (similar to ≥)</li>
<li><code>f(n) = Θ(g(n))</code>: f grows at the same rate as g (similar to =)</li>
<li><code>f(n) = o(g(n))</code>: f grows strictly slower than g (similar to &lt;)</li>
<li><code>f(n) = ω(g(n))</code>: f grows strictly faster than g (similar to &gt;)</li>
</ul>
<h3 id="formal-relationships"><a class="header" href="#formal-relationships">Formal Relationships</a></h3>
<p>If <code>f(n) = O(g(n))</code> and <code>f(n) = Ω(g(n))</code>, then <code>f(n) = Θ(g(n))</code>.<br>This is analogous to: if <code>x ≤ 6</code> and <code>x ≥ 6</code>, then <code>x = 6</code>.</p>
<h3 id="strict-comparisons"><a class="header" href="#strict-comparisons">Strict Comparisons</a></h3>
<ul>
<li><code>2n² + 1 = o(n³)</code> because <code>2n² + 1</code> grows strictly slower than <code>n³</code></li>
<li><code>½n⁵ + n - 1000 = ω(n²)</code> because it grows strictly faster than <code>n²</code></li>
</ul>
<p><strong>Check these:</strong></p>
<ul>
<li>Is <code>100n³ + 4n² + 2 = o(n⁴)</code>? Yes, because the growth rate is strictly less.</li>
<li>Is <code>100n³ + 4n² + 2 = ω(n³)</code>? No, because they have the same growth rate (it’s Θ(n³), not ω(n³)).</li>
</ul>
<h2 id="comparing-growth-rates-using-limits"><a class="header" href="#comparing-growth-rates-using-limits">Comparing Growth Rates Using Limits</a></h2>
<p>We can compare function growth rates using limits:</p>
<ul>
<li>If <code>lim[n→∞] f(n)/g(n) ≤ c₁</code> where <code>c₁ &gt; 0</code>, then <code>f(n) = O(g(n))</code></li>
<li>If <code>lim[n→∞] f(n)/g(n) ≥ c₂</code> where <code>c₂ &gt; 0</code>, then <code>f(n) = Ω(g(n))</code></li>
<li>If <code>c₂ ≤ lim[n→∞] f(n)/g(n) ≤ c₁</code> where <code>c₁, c₂ &gt; 0</code>, then <code>f(n) = Θ(g(n))</code></li>
</ul>
<p><strong>Note</strong>: This is analogous to comparing numbers:</p>
<ul>
<li>If <code>x/y ≤ 1</code>, then <code>x ≤ y</code></li>
<li>If <code>x/y ≥ 1</code>, then <code>x ≥ y</code></li>
<li>If <code>1 ≤ x/y ≤ 1</code> (which means <code>x/y = 1</code>), then <code>x = y</code></li>
</ul>
<h2 id="divide-and-conquer-paradigm"><a class="header" href="#divide-and-conquer-paradigm">Divide and Conquer Paradigm</a></h2>
<p><strong>Divide and Conquer</strong> is a fundamental algorithm design paradigm that involves:</p>
<ol>
<li><strong>Divide</strong>: Break the problem into smaller subproblems</li>
<li><strong>Conquer</strong>: Solve the subproblems recursively</li>
<li><strong>Combine</strong>: Merge the solutions to solve the original problem</li>
</ol>
<h3 id="example-merge-sort"><a class="header" href="#example-merge-sort">Example: Merge Sort</a></h3>
<ol>
<li><strong>Divide</strong>: Split the n-element array into two subarrays of roughly n/2 elements each</li>
<li><strong>Conquer</strong>: Recursively sort the two subarrays using merge sort</li>
<li><strong>Combine</strong>: Merge the two sorted subarrays to produce the final sorted array</li>
</ol>
<h2 id="recurrence-relations"><a class="header" href="#recurrence-relations">Recurrence Relations</a></h2>
<p><strong>Recurrence relations</strong> are equations that define functions in terms of their values on smaller inputs. They’re used to characterize the running times of recursive algorithms.</p>
<h3 id="examples-of-recurrences"><a class="header" href="#examples-of-recurrences">Examples of Recurrences</a></h3>
<ul>
<li><code>T(n) = T(n/2) + c</code></li>
<li><code>T(n) = 3T(n/2) + cn²</code></li>
</ul>
<h3 id="recurrence-for-merge-sort"><a class="header" href="#recurrence-for-merge-sort">Recurrence for Merge Sort</a></h3>
<p>For Merge Sort, the recurrence is:</p>
<pre><code>T(n) = { c               if n = 1
         2T(n/2) + cn    if n &gt; 1 }
</code></pre>
<h2 id="solving-recurrences-iteration-method"><a class="header" href="#solving-recurrences-iteration-method">Solving Recurrences: Iteration Method</a></h2>
<p>The <strong>iteration method</strong> involves repeatedly substituting the recurrence relation until a pattern emerges.</p>
<h3 id="example-1-tn--tn2--c"><a class="header" href="#example-1-tn--tn2--c">Example 1: <code>T(n) = T(n/2) + c</code></a></h3>
<p>Assume <code>n = 2^k</code> for simplicity:</p>
<pre><code>T(n) = T(n/2) + c
     = T(n/4) + c + c
     = T(n/8) + c + c + c
     ...
     = T(1) + c + c + ... + c  [k times]
     = c' + c·k
     = c' + c·log₂n
     = O(log n)
</code></pre>
<p><strong>Explanation</strong>: We can divide n by 2 exactly k = log₂n times before reaching 1.</p>
<h3 id="example-2-tn--2tn2--cn"><a class="header" href="#example-2-tn--2tn2--cn">Example 2: <code>T(n) = 2T(n/2) + cn</code></a></h3>
<pre><code>T(n) = cn + 2T(n/2)
     = cn + 2(c(n/2) + 2T(n/4))
     = cn + cn + 4T(n/4)
     = cn + cn + 4(c(n/4) + 2T(n/8))
     = 3cn + 8T(n/8)
     ...
     = k·cn + 2^k·T(n/2^k)
</code></pre>
<p>When <code>n = 2^k</code>, we have:</p>
<pre><code>T(n) = k·cn + n·T(1)
     = cn·log₂n + n·c'
     = Θ(n log n)
</code></pre>
<h2 id="solving-recurrences-substitution-method"><a class="header" href="#solving-recurrences-substitution-method">Solving Recurrences: Substitution Method</a></h2>
<p>The <strong>substitution method</strong> involves:</p>
<ol>
<li>Guess the form of the solution</li>
<li>Use mathematical induction to prove it</li>
</ol>
<h3 id="example-tn--4tn2--50n"><a class="header" href="#example-tn--4tn2--50n">Example: <code>T(n) = 4T(n/2) + 50n</code></a></h3>
<p><strong>Guess</strong>: <code>T(n) = O(n³)</code></p>
<p><strong>Inductive hypothesis</strong>: Assume <code>T(k) ≤ c·k³</code> for all <code>k &lt; n</code></p>
<p><strong>Proof</strong>:</p>
<pre><code>T(n) = 4T(n/2) + 50n
     ≤ 4c(n/2)³ + 50n
     = (c/2)n³ + 50n
     = cn³ - ((c/2)n³ - 50n)
</code></pre>
<p>We need <code>(c/2)n³ - 50n ≥ 0</code> for <code>T(n) ≤ cn³</code>.<br>This holds when <code>c ≥ 100</code> and <code>n ≥ 1</code>.</p>
<h3 id="tighter-upper-bound"><a class="header" href="#tighter-upper-bound">Tighter Upper Bound</a></h3>
<p>We can actually prove <code>T(n) = O(n²)</code> with a stronger inductive hypothesis.</p>
<p><strong>Inductive hypothesis</strong>: <code>T(k) ≤ c₁k² - c₂k</code> for all <code>k &lt; n</code></p>
<p><strong>Proof</strong>:</p>
<pre><code>T(n) = 4T(n/2) + 50n
     ≤ 4(c₁(n/2)² - c₂(n/2)) + 50n
     = c₁n² - 2c₂n + 50n
     = c₁n² - c₂n - (c₂n - 50n)
</code></pre>
<p>We need <code>c₂n - 50n ≥ 0</code>, which holds when <code>c₂ ≥ 50</code>.<br>Thus <code>T(n) ≤ c₁n² - c₂n ≤ c₁n²</code>, so <code>T(n) = O(n²)</code>.</p>
<hr>
<h2 id="practice-tasks-2"><a class="header" href="#practice-tasks-2">Practice Tasks</a></h2>
<ol>
<li>
<p><strong>Asymptotic Notation Comparisons</strong></p>
<ul>
<li>Prove that <code>5n³ + 2n² + 100 = Θ(n³)</code></li>
<li>Prove that <code>n² log n = ω(n²)</code> but <code>n² log n = o(n³)</code></li>
<li>Determine if <code>√n = Ω(log n)</code> and prove your answer</li>
</ul>
</li>
<li>
<p><strong>Divide and Conquer Application</strong>
Describe how the Divide and Conquer paradigm would apply to:</p>
<ul>
<li>Finding the maximum element in an array</li>
<li>Computing the factorial of a number</li>
<li>Searching for an element in a sorted array (binary search)</li>
</ul>
</li>
<li>
<p><strong>Iteration Method Practice</strong>
Use the iteration method to solve:</p>
<ul>
<li><code>T(n) = T(n/3) + 1</code> with <code>T(1) = 1</code></li>
<li><code>T(n) = 3T(n/4) + n</code></li>
</ul>
</li>
<li>
<p><strong>Substitution Method Practice</strong>
Use the substitution method to prove:</p>
<ul>
<li><code>T(n) = 2T(n/2) + n = O(n log n)</code></li>
<li><code>T(n) = T(n-1) + n = O(n²)</code></li>
</ul>
</li>
<li>
<p><strong>Recurrence Analysis</strong>
For Merge Sort’s recurrence <code>T(n) = 2T(n/2) + cn</code>:</p>
<ul>
<li>Derive the exact solution when <code>n</code> is a power of 2</li>
<li>Explain why the solution is <code>Θ(n log n)</code> even when <code>n</code> is not a power of 2</li>
<li>Compare this with Insertion Sort’s <code>Θ(n²)</code> complexity for large <code>n</code></li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lecture-4-recurrence-relations-continued"><a class="header" href="#lecture-4-recurrence-relations-continued">Lecture 4: Recurrence Relations (Continued)</a></h1>
<h2 id="recap"><a class="header" href="#recap">Recap</a></h2>
<p>This lecture continues the study of <strong>recurrence relations</strong>, which are used to characterize the running times of recursive algorithms. Previously, we covered:</p>
<ul>
<li>Asymptotic Notation (Big-O, Omega, Theta)</li>
<li>The Divide and Conquer paradigm</li>
<li>Basic recurrence relations</li>
</ul>
<h2 id="divide-and-conquer-strategy"><a class="header" href="#divide-and-conquer-strategy">Divide and Conquer Strategy</a></h2>
<p>Many fundamental algorithms use a <strong>divide and conquer</strong> strategy. These algorithms are recursive—they call themselves one or more times to solve subproblems.</p>
<h3 id="the-three-steps"><a class="header" href="#the-three-steps">The Three Steps:</a></h3>
<ol>
<li><strong>Divide</strong>: Break the problem into smaller subproblems.</li>
<li><strong>Conquer</strong>: Solve the subproblems recursively.</li>
<li><strong>Combine</strong>: Merge the solutions of the subproblems to form the solution for the original problem.</li>
</ol>
<h2 id="recurrence-relations-1"><a class="header" href="#recurrence-relations-1">Recurrence Relations</a></h2>
<p>A <strong>recurrence relation</strong> is an equation or inequality that defines a function in terms of its value on smaller inputs. For recursive algorithms, <code>T(n)</code> represents the running time for an input of size <code>n</code>.</p>
<p><strong>Example</strong>:<br><code>T(n) = T(n/2) + c</code><br>This recurrence describes an algorithm that solves a problem of size <code>n</code> by solving one subproblem of half the size and doing a constant amount of additional work.</p>
<h2 id="solving-recurrences-iteration-method-1"><a class="header" href="#solving-recurrences-iteration-method-1">Solving Recurrences: Iteration Method</a></h2>
<p>The <strong>iteration method</strong> involves repeatedly substituting the recurrence into itself until a pattern emerges, then summing the terms.</p>
<h3 id="example-1"><a class="header" href="#example-1">Example:</a></h3>
<p><code>T(n) = T(n/2) + c</code></p>
<p>Assume <code>n = 2^k</code>:</p>
<ol>
<li><code>T(n) = T(n/2) + c</code></li>
<li><code>= [T(n/4) + c] + c = T(n/4) + 2c</code></li>
<li><code>= [T(n/8) + c] + 2c = T(n/8) + 3c</code></li>
<li>Continue <code>k</code> times until we reach <code>T(1)</code>.</li>
<li><code>T(n) = T(1) + k*c</code></li>
</ol>
<p>Since <code>k = log₂n</code>,<br><code>T(n) = T(1) + c*log₂n = O(log n)</code>.</p>
<p><strong>Key Insight</strong>: The number of times we can divide <code>n</code> by 2 until we reach 1 is <code>log₂n</code>.</p>
<h2 id="solving-recurrences-substitution-method-1"><a class="header" href="#solving-recurrences-substitution-method-1">Solving Recurrences: Substitution Method</a></h2>
<p>The <strong>substitution method</strong> involves:</p>
<ol>
<li>Guessing the form of the solution.</li>
<li>Using mathematical induction to verify the guess and find constants.</li>
</ol>
<h3 id="example-1-1"><a class="header" href="#example-1-1">Example:</a></h3>
<p><code>T(n) = 2T(n/2) + 1</code></p>
<p><strong>Guess</strong>: <code>T(n) = O(n)</code><br>Assume <code>T(k) ≤ c*k</code> for all <code>k &lt; n</code>.<br>Then:<br><code>T(n) = 2T(n/2) + 1 ≤ 2*(c*(n/2)) + 1 = c*n + 1</code><br>This is not ≤ <code>c*n</code> for any constant <code>c</code> because of the <code>+1</code>.</p>
<p><strong>Strengthen the guess</strong>: Assume <code>T(k) ≤ c₁*k - c₂</code> for all <code>k &lt; n</code>.<br>Then:<br><code>T(n) = 2T(n/2) + 1 ≤ 2*(c₁*(n/2) - c₂) + 1 = c₁*n - 2c₂ + 1</code><br>We want this ≤ <code>c₁*n - c₂</code>.<br>So we need: <code>-2c₂ + 1 ≤ -c₂</code> ⇒ <code>1 ≤ c₂</code> ⇒ choose <code>c₂ ≥ 1</code> and <code>c₁ &gt; 0</code>.<br>Thus, <code>T(n) = O(n)</code>.</p>
<h2 id="solving-recurrences-recursion-tree-method"><a class="header" href="#solving-recurrences-recursion-tree-method">Solving Recurrences: Recursion-Tree Method</a></h2>
<p>The <strong>recursion-tree method</strong> visualizes the recurrence as a tree where each node represents the cost of a subproblem. We sum the costs at each level and across all levels.</p>
<h3 id="example-1-1-1"><a class="header" href="#example-1-1-1">Example 1:</a></h3>
<p><code>T(n) = T(n/4) + T(n/2) + n²</code></p>
<p><strong>Tree Structure</strong>:</p>
<ul>
<li>Root cost: <code>n²</code></li>
<li>Two children: costs <code>(n/4)²</code> and <code>(n/2)²</code></li>
<li>Their children: costs <code>(n/16)²</code>, <code>(n/8)²</code>, <code>(n/8)²</code>, <code>(n/4)²</code>, etc.</li>
</ul>
<p>The total cost is a geometric series:<br><code>T(n) = n² * [1 + 5/16 + (5/16)² + (5/16)³ + ...]</code></p>
<p>Since <code>5/16 &lt; 1</code>, the series converges to a constant factor.<br>Thus, <code>T(n) = O(n²)</code>.</p>
<h3 id="geometric-series-formula"><a class="header" href="#geometric-series-formula">Geometric Series Formula:</a></h3>
<p>For <code>x ≠ 1</code>:
<code>∑_{k=0}^{n} x^k = (x^{n+1} - 1)/(x - 1)</code></p>
<p>If <code>0 ≤ x &lt; 1</code>, then:
<code>∑_{k=0}^{∞} x^k = 1/(1 - x)</code></p>
<h3 id="example-2"><a class="header" href="#example-2">Example 2:</a></h3>
<p><code>T(n) = 3T(n/4) + cn²</code></p>
<p>The tree has:</p>
<ul>
<li>Root cost: <code>cn²</code></li>
<li>Three children, each with cost <code>c*(n/4)² = c*n²/16</code></li>
<li>Total cost per level decreases geometrically.</li>
<li>The total cost is dominated by the root: <code>T(n) = O(n²)</code>.</li>
</ul>
<h2 id="useful-trick-changing-variables"><a class="header" href="#useful-trick-changing-variables">Useful Trick: Changing Variables</a></h2>
<p>Some recurrences can be simplified by a change of variables.</p>
<h3 id="example-3"><a class="header" href="#example-3">Example:</a></h3>
<p><code>T(n) = 2T(√n) + log n</code></p>
<p>Let <code>m = log n</code> ⇒ <code>n = 2^m</code>.<br>Then: <code>T(2^m) = 2T(2^{m/2}) + m</code></p>
<p>Let <code>S(m) = T(2^m)</code>.<br>Then: <code>S(m) = 2S(m/2) + m</code><br>This is a familiar recurrence with solution <code>S(m) = O(m log m)</code>.</p>
<p>Substitute back:<br><code>T(n) = S(m) = O(m log m) = O(log n * log log n)</code>.</p>
<h2 id="master-method"><a class="header" href="#master-method">Master Method</a></h2>
<p>The <strong>Master Method</strong> provides a cookbook solution for recurrences of the form:</p>
<p><code>T(n) = a * T(n/b) + f(n)</code></p>
<p>where:</p>
<ul>
<li><code>a ≥ 1</code></li>
<li><code>b &gt; 1</code></li>
<li><code>f(n)</code> is asymptotically positive</li>
</ul>
<p>Let <code>g(n) = n^{log_b a}</code>.</p>
<h3 id="three-cases"><a class="header" href="#three-cases">Three Cases:</a></h3>
<p><strong>Case 1</strong>: If <code>f(n) = O(n^{log_b a - ε})</code> for some <code>ε &gt; 0</code>, then:<br><code>T(n) = Θ(n^{log_b a})</code></p>
<p><strong>Case 2</strong>: If <code>f(n) = Θ(n^{log_b a} * log^k n)</code> for some <code>k ≥ 0</code>, then:<br><code>T(n) = Θ(n^{log_b a} * log^{k+1} n)</code></p>
<p><strong>Case 3</strong>: If <code>f(n) = Ω(n^{log_b a + ε})</code> for some <code>ε &gt; 0</code>, <strong>and</strong> if<br><code>a * f(n/b) ≤ c * f(n)</code> for some <code>c &lt; 1</code> and all large <code>n</code> (regularity condition), then:<br><code>T(n) = Θ(f(n))</code></p>
<h3 id="intuitive-summary"><a class="header" href="#intuitive-summary">Intuitive Summary:</a></h3>
<ol>
<li><code>f(n)</code> grows polynomially slower than <code>g(n)</code> → Case 1.</li>
<li><code>f(n)</code> grows at the same rate as <code>g(n)</code>, up to log factors → Case 2.</li>
<li><code>f(n)</code> grows polynomially faster than <code>g(n)</code> and satisfies regularity → Case 3.</li>
</ol>
<h3 id="examples"><a class="header" href="#examples">Examples:</a></h3>
<p><strong>Example 1</strong>: <code>T(n) = 2T(n/2) + n</code></p>
<ul>
<li><code>a=2, b=2, log_b a = 1, g(n)=n</code></li>
<li><code>f(n)=n = Θ(n) = Θ(g(n) * log^0 n)</code> → Case 2 with <code>k=0</code></li>
<li>Solution: <code>T(n) = Θ(n log n)</code></li>
</ul>
<p><strong>Example 2</strong>: <code>T(n) = 2T(n/2) + n²</code></p>
<ul>
<li><code>a=2, b=2, log_b a = 1, g(n)=n</code></li>
<li><code>f(n)=n² = Ω(n^{1+ε})</code> for <code>ε=1</code> → Case 3</li>
<li>Check regularity: <code>2*(n/2)² = n²/2 ≤ c*n²</code> for <code>c=1/2 &lt; 1</code> ✓</li>
<li>Solution: <code>T(n) = Θ(n²)</code></li>
</ul>
<p><strong>Example 3</strong>: <code>T(n) = 2T(n/2) + √n</code></p>
<ul>
<li><code>a=2, b=2, log_b a = 1, g(n)=n</code></li>
<li><code>f(n)=√n = n^{0.5} = O(n^{1-ε})</code> for <code>ε=0.5</code> → Case 1</li>
<li>Solution: <code>T(n) = Θ(n)</code></li>
</ul>
<p><strong>Example 4</strong>: <code>T(n) = 4T(n^{2/3}) + log² n</code><br>This requires a change of variables before applying the Master Method (similar to the earlier trick).</p>
<hr>
<h2 id="practice-tasks-3"><a class="header" href="#practice-tasks-3">Practice Tasks</a></h2>
<ol>
<li>
<p><strong>Iteration Method</strong><br>Use the iteration method to solve:<br><code>T(n) = T(n-1) + n</code> with <code>T(1)=1</code>.<br>Show your steps and give the final asymptotic bound.</p>
</li>
<li>
<p><strong>Substitution Method</strong><br>Use the substitution method to prove that:<br><code>T(n) = T(⌊n/2⌋) + T(⌈n/2⌉) + n</code> is <code>O(n log n)</code>.</p>
</li>
<li>
<p><strong>Recursion Tree</strong><br>Draw the recursion tree for:<br><code>T(n) = 2T(n/3) + n</code>.<br>Determine the cost at each level and sum them to find the asymptotic bound.</p>
</li>
<li>
<p><strong>Change of Variables</strong><br>Solve:<br><code>T(n) = T(√n) + 1</code><br>Hint: Let <code>m = log log n</code>.</p>
</li>
<li>
<p><strong>Master Method</strong><br>Use the Master Method to solve the following recurrences. State which case applies and the solution.
a) <code>T(n) = 4T(n/2) + n</code><br>b) <code>T(n) = 4T(n/2) + n²</code><br>c) <code>T(n) = 4T(n/2) + n³</code></p>
</li>
<li>
<p><strong>Regularity Condition</strong><br>For the recurrence <code>T(n) = 3T(n/2) + n²</code>, verify whether the regularity condition holds for Case 3 of the Master Method.</p>
</li>
<li>
<p><strong>Mixed Method</strong><br>Solve using any method:<br><code>T(n) = 2T(n/4) + √n</code><br>Provide your reasoning and solution.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lecture-5-master-method--strassens-algorithm"><a class="header" href="#lecture-5-master-method--strassens-algorithm">Lecture 5: Master Method &amp; Strassen’s Algorithm</a></h1>
<h2 id="master-method-review"><a class="header" href="#master-method-review">Master Method (Review)</a></h2>
<p>The <strong>Master Method</strong> provides a cookbook solution for solving recurrence relations of the form:</p>
<p>[
T(n) = aT\left(\frac{n}{b}\right) + f(n)
]</p>
<p>where:</p>
<ul>
<li>(a \geq 1) (number of subproblems)</li>
<li>(b &gt; 1) (factor by which input size is reduced)</li>
<li>(f(n)) is asymptotically positive</li>
</ul>
<p>Let:
[
g(n) = n^{\log_b a}
]</p>
<h3 id="three-cases-1"><a class="header" href="#three-cases-1">Three Cases:</a></h3>
<p><strong>Case 1</strong>: If (f(n) = O(g(n)/n^\epsilon)) for some (\epsilon &gt; 0)
[
\text{Then } T(n) = \Theta(g(n))
]</p>
<p><strong>Case 2</strong>: If (f(n) = \Theta(g(n) \log^k n)) for some (k \geq 0)
[
\text{Then } T(n) = \Theta(g(n) \log^{k+1} n)
]</p>
<p><strong>Case 3</strong>: If (f(n) = \Omega(g(n) \cdot n^\epsilon)) for some (\epsilon &gt; 0), <strong>and</strong> the regularity condition holds:
[
af\left(\frac{n}{b}\right) \leq cf(n) \text{ for some } c &lt; 1 \text{ and all sufficiently large } n
]
[
\text{Then } T(n) = \Theta(f(n))
]</p>
<h2 id="master-method-exercises"><a class="header" href="#master-method-exercises">Master Method Exercises</a></h2>
<h3 id="exercise-1"><a class="header" href="#exercise-1">Exercise 1</a></h3>
<p>[
T(n) = 3T\left(\frac{n}{2}\right) + n
]</p>
<p><strong>Solution</strong>:</p>
<ul>
<li>(a = 3), (b = 2)</li>
<li>(g(n) = n^{\log_2 3} \approx n^{1.585})</li>
<li>(f(n) = n)</li>
</ul>
<p>Since (f(n) = n = O(n^{\log_2 3 - \epsilon})) for (\epsilon = \log_2 3 - 1 &gt; 0), this is <strong>Case 1</strong>.
[
T(n) = \Theta(n^{\log_2 3})
]</p>
<h3 id="exercise-2"><a class="header" href="#exercise-2">Exercise 2</a></h3>
<p>[
T(n) = 4T\left(\frac{n}{3}\right) + n^{3/2}
]</p>
<p><strong>Solution</strong>:</p>
<ul>
<li>(a = 4), (b = 3)</li>
<li>(g(n) = n^{\log_3 4} \approx n^{1.262})</li>
<li>(f(n) = n^{3/2})</li>
</ul>
<p>Since (f(n) = n^{3/2} = \Omega(n^{\log_3 4 + \epsilon})) for some (\epsilon &gt; 0) (e.g., (\epsilon = 0.238)), this is <strong>Case 3</strong>.</p>
<p>Check regularity condition:
[
af\left(\frac{n}{b}\right) = 4\left(\frac{n}{3}\right)^{3/2} = \frac{4}{3\sqrt{3}} n^{3/2} \leq c n^{3/2}
]
For (c = \frac{4}{3\sqrt{3}} \approx 0.77 &lt; 1), condition holds.
[
T(n) = \Theta(n^{3/2})
]</p>
<h3 id="exercise-3"><a class="header" href="#exercise-3">Exercise 3</a></h3>
<p>[
T(n) = 4T\left(n^{2/3}\right) + \lg^2 n
]</p>
<p><strong>Solution</strong>:
Use substitution to transform the recurrence:</p>
<ol>
<li>Let (m = \lg n \Rightarrow n = 2^m)</li>
<li>(T(2^m) = 4T(2^{2m/3}) + m^2)</li>
<li>Let (S(m) = T(2^m))</li>
<li>(S(m) = 4S\left(\frac{m}{3/2}\right) + m^2 = 4S\left(\frac{2m}{3}\right) + m^2)</li>
</ol>
<p>Now (a = 4), (b = 3/2)</p>
<ul>
<li>(g(m) = m^{\log_{3/2} 4} \approx m^{3.419})</li>
<li>(f(m) = m^2)</li>
</ul>
<p>Since (f(m) = m^2 = O(m^{\log_{3/2} 4 - \epsilon})) for some (\epsilon &gt; 0), this is <strong>Case 1</strong>.
[
S(m) = \Theta(m^{\log_{3/2} 4})
]
[
T(n) = \Theta((\log n)^{\log_{3/2} 4})
]</p>
<h2 id="matrix-multiplication"><a class="header" href="#matrix-multiplication">Matrix Multiplication</a></h2>
<h3 id="standard-algorithm"><a class="header" href="#standard-algorithm">Standard Algorithm</a></h3>
<p>Given two (n \times n) matrices (A) and (B), their product (C = A \times B) is computed as:</p>
<p>[
c_{ij} = \sum_{k=1}^{n} a_{ik} \cdot b_{kj}
]</p>
<p><strong>Standard Algorithm Complexity</strong>: (\Theta(n^3))</p>
<p><strong>Pseudocode</strong>:</p>
<pre><code>SQUARE-MATRIX-MULTIPLY(A, B):
    n = A.rows
    let C be new n × n matrix
    for i = 1 to n
        for j = 1 to n
            c[i][j] = 0
            for k = 1 to n
                c[i][j] = c[i][j] + a[i][k] * b[k][j]
    return C
</code></pre>
<h2 id="divide-and-conquer-approach"><a class="header" href="#divide-and-conquer-approach">Divide-and-Conquer Approach</a></h2>
<p>Assume (n = 2^k). Partition matrices into 4 submatrices of size (n/2 \times n/2):</p>
<p>[
A = \begin{pmatrix} A_{11} &amp; A_{12} \ A_{21} &amp; A_{22} \end{pmatrix},
\quad B = \begin{pmatrix} B_{11} &amp; B_{12} \ B_{21} &amp; B_{22} \end{pmatrix},
\quad C = \begin{pmatrix} C_{11} &amp; C_{12} \ C_{21} &amp; C_{22} \end{pmatrix}
]</p>
<p>Then:
[
\begin{aligned}
C_{11} &amp;= A_{11}B_{11} + A_{12}B_{21} \
C_{12} &amp;= A_{11}B_{12} + A_{12}B_{22} \
C_{21} &amp;= A_{21}B_{11} + A_{22}B_{21} \
C_{22} &amp;= A_{21}B_{12} + A_{22}B_{22}
\end{aligned}
]</p>
<p><strong>Recurrence</strong>: (T(n) = 8T(n/2) + \Theta(n^2))</p>
<p>By Master Method: (a = 8), (b = 2), (g(n) = n^{\log_2 8} = n^3)
Since (f(n) = \Theta(n^2) = O(n^{3-\epsilon})) for (\epsilon = 1), this is <strong>Case 1</strong>:
[
T(n) = \Theta(n^3)
]
No improvement over standard algorithm.</p>
<h2 id="strassens-algorithm"><a class="header" href="#strassens-algorithm">Strassen’s Algorithm</a></h2>
<p>Strassen discovered a way to multiply matrices using only 7 multiplications instead of 8.</p>
<h3 id="steps"><a class="header" href="#steps">Steps:</a></h3>
<ol>
<li>
<p>Compute 10 sums:
[
\begin{aligned}
S_1 &amp;= B_{12} - B_{22} \
S_2 &amp;= A_{11} + A_{12} \
S_3 &amp;= A_{21} + A_{22} \
S_4 &amp;= B_{21} - B_{11} \
S_5 &amp;= A_{11} + A_{22} \
S_6 &amp;= B_{11} + B_{22} \
S_7 &amp;= A_{12} - A_{22} \
S_8 &amp;= B_{21} + B_{22} \
S_9 &amp;= A_{11} - A_{21} \
S_{10} &amp;= B_{11} + B_{12}
\end{aligned}
]</p>
</li>
<li>
<p>Compute 7 products:
[
\begin{aligned}
P_1 &amp;= A_{11} \cdot S_1 \
P_2 &amp;= S_2 \cdot B_{22} \
P_3 &amp;= S_3 \cdot B_{11} \
P_4 &amp;= A_{22} \cdot S_4 \
P_5 &amp;= S_5 \cdot S_6 \
P_6 &amp;= S_7 \cdot S_8 \
P_7 &amp;= S_9 \cdot S_{10}
\end{aligned}
]</p>
</li>
<li>
<p>Compute result submatrices:
[
\begin{aligned}
C_{11} &amp;= P_5 + P_4 - P_2 + P_6 \
C_{12} &amp;= P_1 + P_2 \
C_{21} &amp;= P_3 + P_4 \
C_{22} &amp;= P_5 + P_1 - P_3 - P_7
\end{aligned}
]</p>
</li>
</ol>
<h3 id="complexity-analysis"><a class="header" href="#complexity-analysis">Complexity Analysis</a></h3>
<p><strong>Recurrence</strong>: (T(n) = 7T(n/2) + \Theta(n^2))</p>
<p>By Master Method: (a = 7), (b = 2), (g(n) = n^{\log_2 7} \approx n^{2.807})
Since (f(n) = \Theta(n^2) = O(n^{\log_2 7 - \epsilon})) for (\epsilon = \log_2 7 - 2 &gt; 0), this is <strong>Case 1</strong>:
[
T(n) = \Theta(n^{\log_2 7}) \approx \Theta(n^{2.807})
]</p>
<h3 id="improved-algorithms"><a class="header" href="#improved-algorithms">Improved Algorithms</a></h3>
<ul>
<li>Coppersmith-Winograd: (O(n^{2.376}))</li>
<li>Virginia Williams: (O(n^{2.3728642}))</li>
<li>François Le Gall: (O(n^{2.3728639}))</li>
</ul>
<hr>
<h2 id="practice-tasks-4"><a class="header" href="#practice-tasks-4">Practice Tasks</a></h2>
<ol>
<li>
<p><strong>Master Method Application</strong>
Solve using the Master Method:
a) (T(n) = 2T(n/4) + \sqrt{n})
b) (T(n) = 4T(n/2) + n^2 \log n)
c) (T(n) = 8T(n/2) + n^3)</p>
</li>
<li>
<p><strong>Matrix Multiplication Analysis</strong>
For the standard divide-and-conquer matrix multiplication:
a) Write the complete recurrence relation including all terms
b) Explain why it doesn’t improve over the naive (\Theta(n^3)) algorithm
c) What would be the complexity if we could reduce the multiplications to 6 instead of 8?</p>
</li>
<li>
<p><strong>Strassen’s Algorithm Verification</strong>
Given:
[
A = \begin{pmatrix} 1 &amp; 2 \ 3 &amp; 4 \end{pmatrix},
\quad B = \begin{pmatrix} 5 &amp; 6 \ 7 &amp; 8 \end{pmatrix}
]
a) Compute the product using Strassen’s algorithm
b) Verify the result matches standard matrix multiplication
c) Count the exact number of scalar multiplications and additions in both methods</p>
</li>
<li>
<p><strong>Recurrence Transformation</strong>
Transform and solve: (T(n) = T(\sqrt{n}) + \log n)
Hint: Use substitution (m = \log n)</p>
</li>
<li>
<p><strong>Algorithm Design</strong>
Design a divide-and-conquer algorithm for multiplying two (n \times n) matrices where (n) is not a power of 2. What modifications are needed? How does this affect the complexity?</p>
</li>
<li>
<p><strong>Comparative Analysis</strong>
For (n = 1024), compare the theoretical number of operations for:
a) Standard matrix multiplication ((\Theta(n^3)))
b) Strassen’s algorithm ((\Theta(n^{\log_2 7})))
c) At what size (n) would Strassen’s algorithm theoretically become faster, assuming constant factors are equal?</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lecture-6-sorting-algorithms-part-i"><a class="header" href="#lecture-6-sorting-algorithms-part-i">Lecture 6: Sorting Algorithms Part I</a></h1>
<h2 id="the-problem-of-sorting"><a class="header" href="#the-problem-of-sorting">The Problem of Sorting</a></h2>
<p><strong>Sorting</strong> is one of the most fundamental problems in computer science. Given a sequence of elements, the goal is to rearrange them in a specific order (typically non-decreasing or non-increasing).</p>
<h3 id="formal-definition"><a class="header" href="#formal-definition">Formal Definition</a></h3>
<p><strong>Input</strong>: A sequence of numbers:<br><code>⟨a₁, a₂, ..., aₙ⟩</code></p>
<p><strong>Output</strong>: A permutation (reordering) of the input sequence:<br><code>⟨a′₁, a′₂, ..., a′ₙ⟩</code><br>such that:<br><code>a′₁ ≤ a′₂ ≤ ... ≤ a′ₙ</code></p>
<p><strong>Example</strong>:<br>Input: <code>8 2 4 9 3 6</code><br>Output: <code>2 3 4 6 8 9</code></p>
<h2 id="key-properties-of-sorting-algorithms"><a class="header" href="#key-properties-of-sorting-algorithms">Key Properties of Sorting Algorithms</a></h2>
<h3 id="stability"><a class="header" href="#stability">Stability</a></h3>
<p>A <strong>stable</strong> sorting algorithm maintains the relative order of records with equal keys (values). If record R appears before record S in the original list and both have the same key, then R will always appear before S in the sorted list.</p>
<p><strong>Why stability matters</strong>: Useful when sorting by multiple criteria (e.g., sort by last name, then by first name).</p>
<h3 id="in-place-sorting"><a class="header" href="#in-place-sorting">In-Place Sorting</a></h3>
<p>An <strong>in-place</strong> sorting algorithm does not require significant additional memory beyond the input array. It transforms the input within the same memory space, possibly using only a small constant amount of extra space for variables.</p>
<p><strong>Trade-off</strong>: In-place algorithms are memory-efficient but may be slower than algorithms that use extra space.</p>
<h2 id="sorting-algorithms-overview"><a class="header" href="#sorting-algorithms-overview">Sorting Algorithms Overview</a></h2>
<p>We will study six major sorting algorithms:</p>
<ol>
<li><strong>Selection Sort</strong> - Simple but inefficient</li>
<li><strong>Insertion Sort</strong> - Efficient for small or nearly sorted arrays</li>
<li><strong>Merge Sort</strong> - Efficient divide-and-conquer algorithm</li>
<li><strong>Quick Sort</strong> - Efficient in practice, uses partitioning</li>
<li><strong>Linear-time Sorting Algorithms</strong> (Radix Sort and Count Sort) - Specialized algorithms</li>
<li><strong>Heap Sort</strong> - Based on heap data structure</li>
</ol>
<h2 id="selection-sort"><a class="header" href="#selection-sort">Selection Sort</a></h2>
<h3 id="basic-idea"><a class="header" href="#basic-idea">Basic Idea</a></h3>
<p>Selection Sort repeatedly finds the minimum element from the unsorted portion and places it at the beginning.</p>
<h3 id="algorithm-steps"><a class="header" href="#algorithm-steps">Algorithm Steps</a></h3>
<ol>
<li>Find the minimum value in the list</li>
<li>Swap it with the value in the first position</li>
<li>Repeat for the remainder of the list (starting at the second position each time)</li>
</ol>
<h3 id="iterative-pseudocode"><a class="header" href="#iterative-pseudocode">Iterative Pseudocode</a></h3>
<pre><code>for i = 1 to n do:
    minIndex = i
    for j = i + 1 to n do:
        if A[j] &lt; A[minIndex] then:
            minIndex = j
    Swap A[i] with A[minIndex]
</code></pre>
<h3 id="recursive-pseudocode"><a class="header" href="#recursive-pseudocode">Recursive Pseudocode</a></h3>
<pre><code>Sort(A, left, right):
    if left ≥ right return
    minIndex = left
    for j = left + 1 to right do:
        if A[j] &lt; A[minIndex] then:
            minIndex = j
    Swap A[left] with A[minIndex]
    Sort(A, left + 1, right)
</code></pre>
<h3 id="time-complexity-analysis"><a class="header" href="#time-complexity-analysis">Time Complexity Analysis</a></h3>
<ul>
<li>Finding the minimum in an array of size <code>n</code> takes <code>Θ(n)</code> time</li>
<li>Recurrence relation: <code>T(n) = n + T(n-1)</code> with <code>T(1) = 1</code></li>
<li>Solution: <code>T(n) = Θ(n²)</code></li>
</ul>
<p><strong>Important</strong>: Selection Sort always takes <code>Θ(n²)</code> time, regardless of input order (even if already sorted).</p>
<h2 id="insertion-sort-1"><a class="header" href="#insertion-sort-1">Insertion Sort</a></h2>
<h3 id="basic-idea-1"><a class="header" href="#basic-idea-1">Basic Idea</a></h3>
<p>Insertion Sort builds the final sorted array one element at a time, similar to how people sort playing cards in their hands.</p>
<h3 id="algorithm-steps-1"><a class="header" href="#algorithm-steps-1">Algorithm Steps</a></h3>
<ol>
<li>Start with the second element</li>
<li>Compare it with elements before it (in the sorted portion)</li>
<li>Shift larger elements to the right</li>
<li>Insert the current element in its correct position</li>
</ol>
<h3 id="pseudocode-2"><a class="header" href="#pseudocode-2">Pseudocode</a></h3>
<pre><code>INSERTION-SORT(A, n):  // A[1...n]
    for j = 2 to n do:
        key = A[j]
        i = j - 1
        while i &gt; 0 and A[i] &gt; key do:
            A[i+1] = A[i]
            i = i - 1
        A[i+1] = key
</code></pre>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<p>Input: <code>8 2 4 9 3 6</code><br>Step-by-step execution:</p>
<ol>
<li><code>2 8 4 9 3 6</code>  (insert 2 before 8)</li>
<li><code>2 4 8 9 3 6</code>  (insert 4 between 2 and 8)</li>
<li><code>2 4 8 9 3 6</code>  (9 is already in correct position)</li>
<li><code>2 3 4 8 9 6</code>  (insert 3 between 2 and 4)</li>
<li><code>2 3 4 6 8 9</code>  (insert 6 between 4 and 8)</li>
</ol>
<h3 id="correctness-proof-by-induction"><a class="header" href="#correctness-proof-by-induction">Correctness Proof (by Induction)</a></h3>
<p><strong>Base case</strong>: For i=1, A[1] is trivially sorted<br><strong>Inductive hypothesis</strong>: Assume A[1…i-1] is sorted at the beginning of i-th iteration<br><strong>Inductive step</strong>: The inner while loop finds the correct position for A[i] and inserts it there, so A[1…i] becomes sorted</p>
<h3 id="time-complexity-analysis-1"><a class="header" href="#time-complexity-analysis-1">Time Complexity Analysis</a></h3>
<p><strong>Worst-case</strong>: Input is reverse sorted<br><code>T(n) = Σ(from j=2 to n) Θ(j) = Θ(n²)</code></p>
<p><strong>Best-case</strong>: Input is already sorted<br><code>T(n) = Θ(n)</code> (each element is already in its correct position)</p>
<p><strong>Average-case</strong>: <code>O(n²)</code></p>
<p><strong>Note</strong>: Insertion Sort runs in time <code>O(n + I)</code>, where I is the number of inversions in the array. This makes it efficient for nearly sorted arrays.</p>
<h2 id="divide-and-conquer-approach-1"><a class="header" href="#divide-and-conquer-approach-1">Divide and Conquer Approach</a></h2>
<p>The quadratic time complexity (<code>n²</code>) of simple sorting algorithms becomes unacceptable for large inputs:</p>
<ul>
<li>Sorting 10⁶ numbers would take hours on even the fastest CPU</li>
</ul>
<p><strong>Better approach</strong>: Divide and conquer</p>
<ul>
<li>Sorting 10⁶ numbers takes &lt;1 millisecond on an average laptop using divide-and-conquer algorithms</li>
</ul>
<h3 id="basic-idea-2"><a class="header" href="#basic-idea-2">Basic Idea</a></h3>
<ol>
<li><strong>Divide</strong>: Split the problem into smaller subproblems</li>
<li><strong>Conquer</strong>: Solve subproblems recursively</li>
<li><strong>Combine</strong>: Merge solutions to solve the original problem</li>
</ol>
<h2 id="merge-sort"><a class="header" href="#merge-sort">Merge Sort</a></h2>
<p>Merge Sort is a classic divide-and-conquer sorting algorithm.</p>
<h3 id="algorithm-steps-2"><a class="header" href="#algorithm-steps-2">Algorithm Steps</a></h3>
<ol>
<li><strong>Divide</strong>: Split the array into two halves</li>
<li><strong>Conquer</strong>: Recursively sort each half</li>
<li><strong>Combine</strong>: Merge the two sorted halves</li>
</ol>
<h3 id="pseudocode-1-1"><a class="header" href="#pseudocode-1-1">Pseudocode</a></h3>
<pre><code>MERGE-SORT(A, left, right):
    if left &lt; right then:
        mid = ⌊(left + right)/2⌋
        MERGE-SORT(A, left, mid)
        MERGE-SORT(A, mid+1, right)
        MERGE(A, left, mid, right)
</code></pre>
<h3 id="merge-subroutine"><a class="header" href="#merge-subroutine">Merge Subroutine</a></h3>
<p>The key to Merge Sort is the <code>MERGE</code> operation, which combines two sorted arrays into one sorted array in linear time.</p>
<p><strong>Time complexity of MERGE</strong>: <code>Θ(n)</code> to merge a total of n elements</p>
<h3 id="example-1-2"><a class="header" href="#example-1-2">Example</a></h3>
<p>Input: <code>20 12 13 11 7 9 2 1</code></p>
<ol>
<li>Divide: <code>[20 12 13 11]</code> and <code>[7 9 2 1]</code></li>
<li>Recursively sort each half</li>
<li>Merge sorted halves</li>
</ol>
<h3 id="time-complexity-analysis-2"><a class="header" href="#time-complexity-analysis-2">Time Complexity Analysis</a></h3>
<p>Recurrence relation:<br><code>T(n) = 2T(n/2) + Θ(n)</code> with <code>T(1) = Θ(1)</code></p>
<p>Solution (using Master Theorem or other methods):<br><code>T(n) = Θ(n log n)</code></p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Always runs in <code>Θ(n log n)</code> time (worst, average, and best cases)</li>
<li>Requires O(n) extra space for merging</li>
<li>Stable sorting algorithm</li>
</ul>
<h2 id="sorting-algorithms-comparison"><a class="header" href="#sorting-algorithms-comparison">Sorting Algorithms Comparison</a></h2>
<h3 id="selection-sort-1"><a class="header" href="#selection-sort-1">Selection Sort</a></h3>
<ul>
<li><strong>Advantages</strong>: Simple to implement, in-place (no extra memory)</li>
<li><strong>Disadvantages</strong>: Always <code>Θ(n²)</code>, even for sorted input</li>
<li><strong>Use case</strong>: Only for very small arrays or educational purposes</li>
</ul>
<h3 id="insertion-sort-1-1"><a class="header" href="#insertion-sort-1-1">Insertion Sort</a></h3>
<ul>
<li><strong>Advantages</strong>: Simple, in-place, efficient for small or nearly sorted arrays (<code>O(n + I)</code> where I is inversions)</li>
<li><strong>Disadvantages</strong>: <code>O(n²)</code> in worst case</li>
<li><strong>Use case</strong>: Small arrays (n ≤ 10-20), nearly sorted data, or as part of more complex algorithms</li>
</ul>
<h3 id="merge-sort-1"><a class="header" href="#merge-sort-1">Merge Sort</a></h3>
<ul>
<li><strong>Advantages</strong>: Guaranteed <code>Θ(n log n)</code> performance, stable</li>
<li><strong>Disadvantages</strong>: Requires O(n) extra space, recursive overhead</li>
<li><strong>Use case</strong>: General-purpose sorting, external sorting (when data doesn’t fit in memory)</li>
</ul>
<h2 id="performance-comparison"><a class="header" href="#performance-comparison">Performance Comparison</a></h2>
<ul>
<li><strong>n² algorithms</strong> (Selection, Insertion): Sorting 10⁶ numbers takes hours</li>
<li><strong>n log n algorithms</strong> (Merge Sort): Sorting 10⁶ numbers takes milliseconds</li>
</ul>
<hr>
<h2 id="practice-tasks-5"><a class="header" href="#practice-tasks-5">Practice Tasks</a></h2>
<h3 id="task-1-algorithm-properties"><a class="header" href="#task-1-algorithm-properties">Task 1: Algorithm Properties</a></h3>
<p>Explain the difference between stable and unstable sorting algorithms. Give an example scenario where stability is crucial.</p>
<h3 id="task-2-selection-sort-trace"><a class="header" href="#task-2-selection-sort-trace">Task 2: Selection Sort Trace</a></h3>
<p>Trace Selection Sort step-by-step on the input: <code>64 25 12 22 11</code>. Show the array after each swap operation.</p>
<h3 id="task-3-insertion-sort-analysis"><a class="header" href="#task-3-insertion-sort-analysis">Task 3: Insertion Sort Analysis</a></h3>
<p>Given the array <code>[5, 2, 4, 6, 1, 3]</code>:</p>
<ol>
<li>Trace Insertion Sort step-by-step</li>
<li>Count the number of inversions in the original array</li>
<li>Verify that the running time is <code>O(n + I)</code> where I is the number of inversions</li>
</ol>
<h3 id="task-4-merge-sort-understanding"><a class="header" href="#task-4-merge-sort-understanding">Task 4: Merge Sort Understanding</a></h3>
<ol>
<li>Explain why Merge Sort requires O(n) extra space</li>
<li>Write the recurrence relation for Merge Sort’s time complexity and solve it</li>
<li>Why is Merge Sort considered a “divide and conquer” algorithm?</li>
</ol>
<h3 id="task-5-algorithm-selection"><a class="header" href="#task-5-algorithm-selection">Task 5: Algorithm Selection</a></h3>
<p>For each scenario below, recommend the most appropriate sorting algorithm (Selection, Insertion, or Merge Sort) and justify your choice:</p>
<ol>
<li>Sorting 20 exam scores</li>
<li>Sorting a list of 1 million customer records by last name</li>
<li>Maintaining a sorted list where new elements are added frequently</li>
<li>Sorting data that is already 95% sorted</li>
</ol>
<h3 id="task-6-implementation-challenge"><a class="header" href="#task-6-implementation-challenge">Task 6: Implementation Challenge</a></h3>
<p>Write pseudocode for the MERGE subroutine that combines two sorted subarrays A[left..mid] and A[mid+1..right] into a single sorted array.</p>
<h3 id="task-7-time-complexity-proof"><a class="header" href="#task-7-time-complexity-proof">Task 7: Time Complexity Proof</a></h3>
<p>Prove that Selection Sort has time complexity Θ(n²) by:</p>
<ol>
<li>Writing the recurrence relation</li>
<li>Solving it mathematically</li>
<li>Explaining why it’s the same for best, worst, and average cases</li>
</ol>
<h3 id="task-8-stability-analysis"><a class="header" href="#task-8-stability-analysis">Task 8: Stability Analysis</a></h3>
<p>Determine whether each algorithm is stable or unstable, and explain why:</p>
<ol>
<li>Selection Sort</li>
<li>Insertion Sort</li>
<li>Merge Sort</li>
</ol>
<h3 id="task-9-real-world-application"><a class="header" href="#task-9-real-world-application">Task 9: Real-world Application</a></h3>
<p>A mobile app needs to sort contacts by last name, then by first name. Which sorting property (stability) is important here, and why? Which of the three algorithms studied would be appropriate?</p>
<h3 id="task-10-performance-calculation"><a class="header" href="#task-10-performance-calculation">Task 10: Performance Calculation</a></h3>
<p>Estimate how long it would take to sort 1 million integers using:</p>
<ol>
<li>Selection Sort (assume 10⁸ operations per second)</li>
<li>Merge Sort (assume 10⁸ operations per second)
Compare the results and explain the practical implications.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lecture-7-sorting-algorithms---lower-bounds--quick-sort"><a class="header" href="#lecture-7-sorting-algorithms---lower-bounds--quick-sort">Lecture 7: Sorting Algorithms - Lower Bounds &amp; Quick Sort</a></h1>
<h2 id="sorting-algorithms-review"><a class="header" href="#sorting-algorithms-review">Sorting Algorithms Review</a></h2>
<p>In previous lectures, we discussed several sorting algorithms:</p>
<ul>
<li><strong>Selection Sort</strong>: Simple but always Θ(n²)</li>
<li><strong>Insertion Sort</strong>: Efficient for small/nearly-sorted data, worst-case Θ(n²)</li>
<li><strong>Merge Sort</strong>: Always Θ(n log n) but requires additional memory</li>
</ul>
<p>All these algorithms are <strong>comparison-based</strong>: they only use comparisons between elements to determine their order.</p>
<h2 id="lower-bounds-for-comparison-based-sorting"><a class="header" href="#lower-bounds-for-comparison-based-sorting">Lower Bounds for Comparison-Based Sorting</a></h2>
<h3 id="comparison-decision-tree-model"><a class="header" href="#comparison-decision-tree-model">Comparison (Decision Tree) Model</a></h3>
<p>The <strong>decision tree model</strong> represents comparison-based sorting algorithms as binary trees:</p>
<ul>
<li>Each <strong>internal node</strong> represents a comparison <code>A[i] ≤ A[j]</code></li>
<li>Each <strong>leaf</strong> represents a possible sorted permutation</li>
<li>The <strong>execution path</strong> from root to leaf shows the sequence of comparisons made</li>
</ul>
<p><strong>Example</strong>: Decision tree for sorting 3 elements A[0], A[1], A[2] has leaves for all 3! = 6 possible permutations.</p>
<h3 id="theorem-any-decision-tree-sorting-n-elements-has-height-ωn-log-n"><a class="header" href="#theorem-any-decision-tree-sorting-n-elements-has-height-ωn-log-n">Theorem: Any decision tree sorting n elements has height Ω(n log n)</a></h3>
<p><strong>Proof</strong>:</p>
<ol>
<li>There are n! possible permutations of n elements (each requires a unique leaf)</li>
<li>A binary tree of height h has at most 2ʰ leaves</li>
<li>Therefore: 2ʰ ≥ n!</li>
<li>Taking logs: h ≥ log(n!)</li>
<li>Using Stirling’s approximation: n! &gt; (n/e)ⁿ</li>
<li>Therefore: h ≥ log((n/e)ⁿ) = n log n - n log e</li>
<li>Thus: h = Ω(n log n)</li>
</ol>
<p><strong>Implications</strong>:</p>
<ul>
<li>Merge Sort is <strong>optimal</strong> for comparison-based sorting with Θ(n log n) worst-case</li>
<li>To beat Ω(n log n), we must use more than just comparisons (e.g., exploit integer properties)</li>
<li>Algorithms like Radix Sort and Counting Sort can achieve Θ(n) for integers with bounded range</li>
</ul>
<h2 id="quick-sort"><a class="header" href="#quick-sort">Quick Sort</a></h2>
<p>Quick Sort is a <strong>divide-and-conquer</strong>, <strong>in-place</strong> sorting algorithm that is often faster than Merge Sort in practice.</p>
<h3 id="divide-and-conquer-strategy-1"><a class="header" href="#divide-and-conquer-strategy-1">Divide-and-Conquer Strategy</a></h3>
<ol>
<li>
<p><strong>Divide</strong>: Partition the array around a pivot element x:</p>
<ul>
<li>Left subarray: elements ≤ x</li>
<li>Right subarray: elements ≥ x</li>
</ul>
</li>
<li>
<p><strong>Conquer</strong>: Recursively sort the two subarrays</p>
</li>
<li>
<p><strong>Combine</strong>: Trivial (subarrays are already in place)</p>
</li>
</ol>
<h3 id="partitioning-subroutine"><a class="header" href="#partitioning-subroutine">Partitioning Subroutine</a></h3>
<p>The key to Quick Sort is the linear-time partitioning algorithm:</p>
<pre><code>PARTITION(A, p, q)  // A[p...q]
    x = A[p]        // pivot element
    i = p
    for j = p+1 to q
        if A[j] ≤ x
            i = i + 1
            exchange A[i] with A[j]
    exchange A[p] with A[i]
    return i
</code></pre>
<p><strong>Invariant maintained during partitioning</strong>:</p>
<ul>
<li>Elements A[p…i] are ≤ x</li>
<li>Elements A[i+1…j-1] are &gt; x</li>
<li>Elements A[j…q] are unprocessed</li>
</ul>
<h3 id="example-of-partitioning"><a class="header" href="#example-of-partitioning">Example of Partitioning</a></h3>
<p>Array: <code>6 10 13 5 8 3 2 11</code> (pivot = 6)</p>
<p>Step-by-step:</p>
<ol>
<li><code>6 10 13 5 8 3 2 11</code>  (j=1, 10&gt;6, no swap)</li>
<li><code>6 10 13 5 8 3 2 11</code>  (j=2, 13&gt;6, no swap)</li>
<li><code>6 5 13 10 8 3 2 11</code>  (j=3, 5≤6, swap with 10)</li>
<li><code>6 5 3 10 8 13 2 11</code>  (j=4, 3≤6, swap with 13)</li>
<li><code>6 5 3 2 8 13 10 11</code>  (j=5, 2≤6, swap with 8)</li>
<li>Final: <code>2 5 3 6 8 13 10 11</code> (swap pivot with last ≤ element)</li>
</ol>
<h3 id="quick-sort-pseudocode"><a class="header" href="#quick-sort-pseudocode">Quick Sort Pseudocode</a></h3>
<pre><code>QUICKSORT(A, p, r)
    if p &lt; r
        q = PARTITION(A, p, r)
        QUICKSORT(A, p, q-1)
        QUICKSORT(A, q+1, r)
</code></pre>
<p>Initial call: <code>QUICKSORT(A, 1, n)</code></p>
<h2 id="running-time-analysis-1"><a class="header" href="#running-time-analysis-1">Running Time Analysis</a></h2>
<h3 id="worst-case-θn²"><a class="header" href="#worst-case-θn²">Worst-Case: Θ(n²)</a></h3>
<ul>
<li>Occurs when partition is maximally unbalanced</li>
<li>Example: Input already sorted or reverse sorted</li>
<li>Recurrence: T(n) = T(n-1) + Θ(n) = Θ(n²)</li>
</ul>
<h3 id="best-case-θn-log-n"><a class="header" href="#best-case-θn-log-n">Best-Case: Θ(n log n)</a></h3>
<ul>
<li>Occurs when partition splits array exactly in half</li>
<li>Recurrence: T(n) = 2T(n/2) + Θ(n) = Θ(n log n)</li>
</ul>
<h3 id="average-case-θn-log-n"><a class="header" href="#average-case-θn-log-n">Average-Case: Θ(n log n)</a></h3>
<ul>
<li>Expected performance with random input</li>
<li>Even with 1:9 splits: T(n) = T(n/10) + T(9n/10) + Θ(n) = Θ(n log n)</li>
<li>Intuition: Random pivots usually yield reasonably balanced partitions</li>
</ul>
<h2 id="comparison-quick-sort-vs-merge-sort"><a class="header" href="#comparison-quick-sort-vs-merge-sort">Comparison: Quick Sort vs Merge Sort</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Quick Sort</th><th>Merge Sort</th></tr>
</thead>
<tbody>
<tr><td>Worst-case</td><td>Θ(n²)</td><td>Θ(n log n)</td></tr>
<tr><td>Average-case</td><td>Θ(n log n)</td><td>Θ(n log n)</td></tr>
<tr><td>In-place</td><td>Yes</td><td>No (requires O(n) extra)</td></tr>
<tr><td>Stable</td><td>Typically no</td><td>Yes</td></tr>
<tr><td>Practical speed</td><td>2-3x faster (cache efficient)</td><td>Slower (memory movement)</td></tr>
<tr><td>Optimal</td><td>No (worst-case Θ(n²))</td><td>Yes (worst-case optimal)</td></tr>
</tbody>
</table>
</div>
<h2 id="practical-considerations"><a class="header" href="#practical-considerations">Practical Considerations</a></h2>
<ol>
<li>
<p><strong>Pivot selection</strong> matters:</p>
<ul>
<li>First/last element: vulnerable to sorted inputs</li>
<li>Random element: good expected performance</li>
<li>Median-of-three: better protection against bad splits</li>
</ul>
</li>
<li>
<p><strong>Small subarrays</strong>: For small n (e.g., n ≤ 10), Insertion Sort is often faster</p>
</li>
<li>
<p><strong>Stack depth</strong>: Naive recursion can cause O(n) stack depth in worst-case</p>
</li>
<li>
<p><strong>Equal elements</strong>: Standard implementation is not stable</p>
</li>
</ol>
<hr>
<h2 id="practice-tasks-6"><a class="header" href="#practice-tasks-6">Practice Tasks</a></h2>
<ol>
<li>
<p><strong>Decision Tree Analysis</strong></p>
<ul>
<li>Draw the decision tree for sorting 3 elements (A, B, C)</li>
<li>What is the minimum height required? Verify it satisfies h ≥ log(3!)</li>
<li>How many leaves does a decision tree for sorting 4 elements need?</li>
</ul>
</li>
<li>
<p><strong>Partitioning Execution</strong></p>
<ul>
<li>Trace the PARTITION algorithm on array <code>[9, 7, 5, 11, 12, 2, 14, 3, 10, 6]</code> with pivot = 9</li>
<li>Show the array after each iteration of the for loop</li>
<li>Identify the final position of the pivot</li>
</ul>
</li>
<li>
<p><strong>Quick Sort Analysis</strong></p>
<ul>
<li>What input produces the worst-case running time for Quick Sort?</li>
<li>Prove that T(n) = T(n-1) + Θ(n) yields Θ(n²)</li>
<li>Explain why average-case Quick Sort is Θ(n log n) even with uneven splits</li>
</ul>
</li>
<li>
<p><strong>Comparative Analysis</strong></p>
<ul>
<li>When would you choose Quick Sort over Merge Sort?</li>
<li>When would you choose Merge Sort over Quick Sort?</li>
<li>How does Quick Sort’s in-place property affect its cache performance?</li>
</ul>
</li>
<li>
<p><strong>Algorithm Design</strong></p>
<ul>
<li>Design a modified PARTITION that handles duplicate elements efficiently</li>
<li>Propose a pivot selection strategy to avoid worst-case behavior</li>
<li>How could you make Quick Sort stable? What would be the trade-offs?</li>
</ul>
</li>
<li>
<p><strong>Complexity Proof</strong></p>
<ul>
<li>Using the decision tree model, prove that any comparison-based sorting algorithm requires at least Ω(n log n) comparisons in the worst case</li>
<li>Why doesn’t this lower bound apply to Radix Sort or Counting Sort?</li>
</ul>
</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>


        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
